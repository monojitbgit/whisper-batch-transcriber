# ğŸ™ï¸ Whisper Batch Transcriber

This Python script automates **speech-to-text transcription** for multiple audio files using [OpenAI Whisper](https://github.com/openai/whisper).  
It scans an `audio/` folder for supported files, transcribes them, and saves all results into a single CSV file.

---
<br>

## ğŸš€ Features

- ğŸ” Automatically detects all audio files in the `audio/` folder
- ğŸ§  Uses OpenAIâ€™s **Whisper** model for accurate transcription
- ğŸ—£ï¸ Supports multiple audio formats (`.mp3`, `.wav`, `.m4a`, `.flac`, `.ogg`, `.webm`)  
- ğŸ’¾ Saves transcripts neatly in a `transcripts.csv` file
- âš¡ Flushes results to disk after every file (prevents data loss)  
- â±ï¸ Displays progress and elapsed time  

---
<br>

## ğŸ“¦ Requirements

- **Python 3.8+**
- **Dependencies:**
  ```bash
  pip install openai-whisper

  pip install torch  # required backend for Whisper
  ```
(If using GPU, install the CUDA-enabled version of PyTorch)

---
<br>

## ğŸ§© Installing the Open-Source Whisper Module (Manual Method)

If you want to manually install Whisper from source, follow these steps:

 Download Whisper from GitHub:
ğŸ”— https://github.com/openai/whisper

Extract the downloaded ZIP and navigate to the folder:
whisper-main

Install locally using PowerShell or VS Code terminal:

  ```bash
cd C:\Users\User\Downloads\whisper-main
pip install -e .
  ```

Alternatively, install dependencies manually:

  ```bash
pip install -r requirements.txt
  ```
OR

  ```bash
pip install openai-whisper
  ```
âœ… This automatically installs all required dependencies, including:
  ```bash
numba
numpy
torch
tqdm
more-itertools
tiktoken
  ```

âš ï¸ Note:
On Linux (x86_64), it may also install triton>=2.0.0 if supported by your system.
<br>You do not need to install these manually unless building from source.

---
<br>

## ğŸï¸ Installing FFmpeg (Required for Audio Processing)

Download FFmpeg from:
ğŸ”— https://ffmpeg.org/download.html#build-windows

Extract the files and move the folder to:

  ```bash
C:\ffmpeg
  ```

Add FFmpeg to your system environment variables:
<br>Go to: Control Panel â†’ System â†’ Advanced System Settings â†’ Environment Variables
<br>Under User variables, find Path

Add:

  ```bash
C:\ffmpeg\bin
  ```

Verify installation:

  ```bash
ffmpeg -version
  ```

---
<br>

## ğŸ“ Folder Structure
project/
<br>â”‚
<br>â”œâ”€â”€ audio/              # Folder containing your audio files
<br>â”‚   â”œâ”€â”€ example1.mp3
<br>â”‚   â”œâ”€â”€ example2.wav
<br>â”‚
<br>â”œâ”€â”€ transcripts.csv     # Generated output (after running the script)
<br>â”‚
<br>â””â”€â”€ transcribe.py       # The main script

---
<br>

## âš™ï¸ Usage
Clone the repository:

  ```bash
git clone https://github.com/monojitbgit/whisper-batch-transcriber.git
cd whisper-batch-transcriber
  ```

Place your audio files inside the audio/ folder.

Run the script:
  ```bash
python transcribe.py
  ```

---
<br>

## ğŸ§  Model Details

By default, the script loads the medium Whisper model:
  ```bash
model = whisper.load_model("medium")
  ```

To use a different model:
  ```bash
model = whisper.load_model("small")
  ```

Change the transcription language by adjusting:
  ```bash
transcribe_file(file_path, language="en")
  ```
(Replace "en" with your desired ISO language code.)

---
<br>

## ğŸ§° Notes

Whisper runs locally â€” no API key required
<br>For faster performance, use a GPU (CUDA-compatible)
<br>Large models need more memory â€” try small or base if you get CUDA out of memory

---
<br>

## âš–ï¸ Whisper Model Comparison

| Option | Speed | Accuracy | Model | â±ï¸ CPU (Intel-i7/Ryzen-7) | âš™ï¸ GPU (RTX 3060/3090/4090) | Size | Speed (Relative) | Accuracy (Relative) | ğŸŒ Multilingual Support |
|--------|--------|-----------|--------|------------------------------|-----------------------------|-------|------------------|----------------------|--------------------------|
| Use `"tiny"` | Fast | Low accuracy | `tiny` | ~30 sec | ~5â€“10 sec | 75 MB | â­â­â­â­ (Fastest) | â­ (Basic) | âœ… Yes |
| Use `"base"` | Fast | Low accuracy | `base` | ~1 min | ~10â€“20 sec | 142 MB | â­â­â­ (Fast) | â­â­ (Good) | âœ… Yes |
| Use `"small"` with CPU optimizations | Faster | Good | `small` | ~2â€“3 min | ~20â€“30 sec | 466 MB | â­â­ (Medium) | â­â­â­ (Better) | âœ… Yes |
| Use `"medium"` after reducing file size | Slow | Better | `medium` | ~5â€“6 min | ~40â€“60 sec | 1.5 GB | â­ (Slower) | â­â­â­â­ (Very Good) | âœ… Yes |
| Use Google Colab GPU (`medium` / `large`) | Super Fast | Best | `large` | ~10â€“15 min | ~1.5â€“2 min | 3.0 GB | â­ (Slowest) | â­â­â­â­â­ (Best) | âœ… Yes |

---
<br>

## ğŸ’¡ Recommendations

- ğŸ§  **For quick tests:** Use `tiny` or `base`  
- âš–ï¸ **For balance (speed + accuracy):** Use `small`  
- ğŸ—£ï¸ **For high-quality transcription:** Use `medium`  
- ğŸš€ **For best performance:** Use `large` on GPU or Google Colab  

---
<br>

## ğŸ§¾ License

This project is licensed under the MIT License.
<br>Youâ€™re free to use, modify, and distribute it with attribution.
